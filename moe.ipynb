{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baisc MOE\n",
    "* Single `nn.Linear` layer as the expert module\n",
    "* `nn.Linear` layer as the dense router\n",
    "* input with shape `(b, feature_in)`, output with shape `(b, feature_out)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicExpert(nn.Module):\n",
    "    def __init__(self, feature_in, feature_out):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(feature_in, feature_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicMOE(nn.Module):\n",
    "    def __init__(self, feature_in, feature_out, num_experts):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Linear(feature_in, num_experts)\n",
    "        # output shape (batch_size, num_experts)\n",
    "        self.experts = nn.ModuleList(\n",
    "            BasicExpert(\n",
    "                feature_in, feature_out\n",
    "            ) for _ in range(num_experts)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape (batch_size, feature_in)\n",
    "        expert_weights = self.gate(x)\n",
    "        expert_out_list = [\n",
    "            expert(x).unsqueeze(1) for expert in self.experts\n",
    "        ] # each expert output in shape (batch_size, feature_out), unsqueeze to (batch_size, 1, feature_out)\n",
    "\n",
    "        expert_output = torch.concat(expert_out_list, dim=1)\n",
    "        # expert_output shape (b, num_experts, feature_out)\n",
    "\n",
    "        #expert weights\n",
    "        expert_weights = F.softmax(expert_weights, dim=1)\n",
    "        expert_weights = expert_weights.unsqueeze(1)\n",
    "        # exopert_weights shape (b, 1, num_experts)\n",
    "\n",
    "        output = expert_weights @ expert_output\n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 128])\n"
     ]
    }
   ],
   "source": [
    "def test_basic_moe():\n",
    "    x = torch.rand(10, 512)\n",
    "    basic_moe = BasicMOE(512, 128, 4)\n",
    "    output = basic_moe(x)\n",
    "    print(output.shape)\n",
    "\n",
    "test_basic_moe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse MOE\n",
    "* top-K router\n",
    "* input with shape `(b, seq_len, hidden_dim)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MOEConfig:\n",
    "    def __init__(self, hidden_dim, expert_num, top_k, shared_expert_num=2):\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.expert_num = expert_num\n",
    "        self.top_k = top_k\n",
    "        self.shared_expert_num = shared_expert_num\n",
    "\n",
    "class MOERouter(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Linear(config.hidden_dim, config.expert_num)\n",
    "        self.top_k = config.top_k\n",
    "        self.expert_num = config.expert_num\n",
    "    \n",
    "    def forward(self, x):\n",
    "        router_logits = self.gate(x)\n",
    "        # router_logits shape (b*seq_len, expert_num)\n",
    "        router_probs = F.softmax(router_logits, dim=-1)\n",
    "\n",
    "        router_weights, top_k_indices = router_probs.topk(k=self.top_k, dim=-1)\n",
    "        # router_weight & top_k_indices shape (b*seq_len, top_k)\n",
    "\n",
    "        router_weights = router_weights / router_weights.sum(dim=-1, keepdim=True)\n",
    "\n",
    "        expert_masks = F.one_hot(top_k_indices, self.expert_num)\n",
    "        # expert_mask shape (b*seq_len, top_k, expert_num)\n",
    "        expert_masks = expert_masks.permute(2, 1, 0)\n",
    "        # expert_mask shape (expert_num, top_k, b*seq_len)\n",
    "        \n",
    "        return router_logits, router_weights, top_k_indices, expert_masks\n",
    "\n",
    "\n",
    "class SparseMOE(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.top_k = config.top_k\n",
    "        self.hidden_dim = config.hidden_dim\n",
    "        self.expert_num = config.expert_num\n",
    "\n",
    "        self.experts = nn.ModuleList([\n",
    "            BasicExpert(\n",
    "                config.hidden_dim,\n",
    "                config.hidden_dim\n",
    "            ) for _ in range(config.expert_num)\n",
    "        ])\n",
    "        self.router = MOERouter(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape (b, seq_len, hidden_dim)\n",
    "        batch_size, seq_len, hidden_dim = x.shape\n",
    "\n",
    "        # reshape to (b*seq_len, hidden_dim) for token level calculation\n",
    "        x = x.reshape(-1, hidden_dim)\n",
    "\n",
    "        router_logtis, router_weights, top_k_indices, expert_masks = self.router(x)\n",
    "\n",
    "        final_x = torch.zeros((batch_size*seq_len, hidden_dim), dtype=x.dtype, device=x.device)\n",
    "\n",
    "        for expert_idx in range(self.expert_num):\n",
    "            expert_layer = self.experts[expert_idx]\n",
    "\n",
    "            current_expert_mask = expert_masks[expert_idx]\n",
    "            # current_expert_mask shape (top_k, b*seq_len)\n",
    "            \n",
    "            top_idx, token_idx = torch.where(current_expert_mask)\n",
    "            # top_idx shape: (top_k), token_idx shape: (b*seq_len)\n",
    "\n",
    "            current_x = x[token_idx]\n",
    "            # current_x shape (selected_token_num, hidden_dim)\n",
    "            current_x = expert_layer(current_x)\n",
    "            current_token_router_weight = router_weights[token_idx, top_idx]\n",
    "            # current_token_router_weight shape (selected_token_num)\n",
    "            current_token_router_weight = current_token_router_weight.unsqueeze(-1)\n",
    "            # current_token_router_weight shape (selected_token_num, 1)\n",
    "\n",
    "            current_x *= current_token_router_weight\n",
    "\n",
    "            final_x.index_add_(0, token_idx, current_x)\n",
    "\n",
    "        final_x = final_x.reshape(batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        return final_x, router_logtis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 100, 512]) torch.Size([1000, 16])\n"
     ]
    }
   ],
   "source": [
    "def test_SparseMOE():\n",
    "    x = torch.rand(10, 100, 512)\n",
    "    config = MOEConfig(512, 16, 4)\n",
    "    sparse_moe = SparseMOE(config)\n",
    "    output = sparse_moe(x)\n",
    "    print(output[0].shape, output[1].shape)\n",
    "\n",
    "test_SparseMOE()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepSeek MOE\n",
    "* include shared expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedExpertMOE(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.routed_expert_moe = SparseMOE(config)\n",
    "        self.shared_experts = nn.ModuleList(\n",
    "            [\n",
    "                BasicExpert(self.config.hidden_dim, self.config.hidden_dim)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (b, seq_len, hidden_dim)\n",
    "        b, seq_len, hidden_dim = x.shape\n",
    "\n",
    "        shared_expert_output_list = [expert(x) for expert in self.shared_experts]\n",
    "        shared_expert_output = torch.stack(shared_expert_output_list, dim=0)\n",
    "        # shared_expert_output shape (shared_expert_num, b, seq_len, hidden_dim)\n",
    "\n",
    "        shared_expert_output = shared_expert_output.sum(dim=0)\n",
    "        # shared_expert_output shape (b, seq_len, hidden_dim)\n",
    "        spared_moe_out, router_logits = self.routed_expert_moe(x)\n",
    "\n",
    "        output = spared_moe_out + shared_expert_output\n",
    "\n",
    "        return output, router_logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 100, 512]) torch.Size([1000, 16])\n"
     ]
    }
   ],
   "source": [
    "def test_SharedExpertMOE():\n",
    "    x = torch.rand(10, 100, 512)\n",
    "    config = MOEConfig(512, 16, 4)\n",
    "    sparse_moe = SharedExpertMOE(config)\n",
    "    output = sparse_moe(x)\n",
    "    print(output[0].shape, output[1].shape)\n",
    "\n",
    "test_SharedExpertMOE()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load balancing loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_load_balancing_loss(router_logits, expert_num, top_k):\n",
    "    router_probs = F.softmax(router_logits, dim=-1)\n",
    "    # (b*seq_len, expert_num)\n",
    "    \n",
    "    _, selected_expert = torch.topk(router_probs, k=top_k, dim=-1)\n",
    "    # (b*seq_len, top_k)\n",
    "    expert_mask = F.one_hot(selected_expert, expert_num).to(torch.float)\n",
    "    # (b*seq_len, top_k, expert_num)\n",
    "    actual_load = expert_mask.mean(dim=0)\n",
    "    # (top_k, expert_num)\n",
    "\n",
    "    aux_loss = torch.sum(actual_load * router_probs.mean(dim=0)) * expert_num\n",
    "\n",
    "    z_loss = torch.mean(router_logits**2)\n",
    "    z_loss_weight = 0.01\n",
    "\n",
    "    return aux_loss + z_loss * z_loss_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 1.7621 (MSE: 1.5131, Aux: 2.0110)\n",
      "Batch 10, Loss: 1.7215 (MSE: 1.4367, Aux: 2.0063)\n",
      "Batch 20, Loss: 1.6744 (MSE: 1.3442, Aux: 2.0047)\n",
      "Batch 30, Loss: 1.6353 (MSE: 1.2658, Aux: 2.0049)\n",
      "Batch 40, Loss: 1.6094 (MSE: 1.2162, Aux: 2.0025)\n",
      "Batch 50, Loss: 1.5835 (MSE: 1.1636, Aux: 2.0034)\n",
      "Batch 60, Loss: 1.5631 (MSE: 1.1247, Aux: 2.0015)\n",
      "Batch 70, Loss: 1.5526 (MSE: 1.1025, Aux: 2.0027)\n",
      "Batch 80, Loss: 1.5522 (MSE: 1.1022, Aux: 2.0022)\n",
      "Batch 90, Loss: 1.5279 (MSE: 1.0545, Aux: 2.0012)\n",
      "Batch 100, Loss: 1.5264 (MSE: 1.0506, Aux: 2.0022)\n",
      "Batch 110, Loss: 1.5311 (MSE: 1.0607, Aux: 2.0014)\n",
      "Batch 120, Loss: 1.5301 (MSE: 1.0579, Aux: 2.0022)\n",
      "Batch 130, Loss: 1.5235 (MSE: 1.0443, Aux: 2.0027)\n",
      "Batch 140, Loss: 1.5193 (MSE: 1.0363, Aux: 2.0024)\n",
      "Batch 150, Loss: 1.5212 (MSE: 1.0413, Aux: 2.0010)\n",
      "Batch 160, Loss: 1.5230 (MSE: 1.0424, Aux: 2.0037)\n",
      "Batch 170, Loss: 1.5134 (MSE: 1.0244, Aux: 2.0025)\n",
      "Batch 180, Loss: 1.5135 (MSE: 1.0214, Aux: 2.0055)\n",
      "Batch 190, Loss: 1.5164 (MSE: 1.0314, Aux: 2.0013)\n",
      "Batch 200, Loss: 1.5127 (MSE: 1.0216, Aux: 2.0039)\n",
      "Batch 210, Loss: 1.5155 (MSE: 1.0297, Aux: 2.0013)\n",
      "Batch 220, Loss: 1.5111 (MSE: 1.0205, Aux: 2.0018)\n",
      "Batch 230, Loss: 1.5099 (MSE: 1.0183, Aux: 2.0015)\n",
      "Batch 240, Loss: 1.5145 (MSE: 1.0275, Aux: 2.0015)\n",
      "Batch 250, Loss: 1.5046 (MSE: 1.0074, Aux: 2.0019)\n",
      "Batch 260, Loss: 1.5077 (MSE: 1.0149, Aux: 2.0004)\n",
      "Batch 270, Loss: 1.5080 (MSE: 1.0138, Aux: 2.0022)\n",
      "Batch 280, Loss: 1.5021 (MSE: 1.0037, Aux: 2.0005)\n",
      "Batch 290, Loss: 1.5075 (MSE: 1.0139, Aux: 2.0010)\n",
      "Batch 300, Loss: 1.5006 (MSE: 0.9985, Aux: 2.0026)\n",
      "Batch 310, Loss: 1.4994 (MSE: 0.9983, Aux: 2.0005)\n",
      "Batch 320, Loss: 1.5017 (MSE: 1.0027, Aux: 2.0006)\n",
      "Batch 330, Loss: 1.5177 (MSE: 1.0345, Aux: 2.0009)\n",
      "Batch 340, Loss: 1.5032 (MSE: 1.0051, Aux: 2.0012)\n",
      "Batch 350, Loss: 1.5056 (MSE: 1.0103, Aux: 2.0010)\n",
      "Batch 360, Loss: 1.5045 (MSE: 1.0077, Aux: 2.0013)\n",
      "Batch 370, Loss: 1.4998 (MSE: 0.9976, Aux: 2.0020)\n",
      "Batch 380, Loss: 1.5148 (MSE: 1.0290, Aux: 2.0006)\n",
      "Batch 390, Loss: 1.5111 (MSE: 1.0206, Aux: 2.0016)\n",
      "Batch 400, Loss: 1.5003 (MSE: 1.0003, Aux: 2.0002)\n",
      "Batch 410, Loss: 1.5031 (MSE: 1.0058, Aux: 2.0004)\n",
      "Batch 420, Loss: 1.5071 (MSE: 1.0136, Aux: 2.0005)\n",
      "Batch 430, Loss: 1.4992 (MSE: 0.9979, Aux: 2.0005)\n",
      "Batch 440, Loss: 1.4994 (MSE: 0.9973, Aux: 2.0014)\n",
      "Batch 450, Loss: 1.5059 (MSE: 1.0108, Aux: 2.0010)\n",
      "Batch 460, Loss: 1.5060 (MSE: 1.0109, Aux: 2.0012)\n",
      "Batch 470, Loss: 1.5052 (MSE: 1.0094, Aux: 2.0010)\n",
      "Batch 480, Loss: 1.4991 (MSE: 0.9979, Aux: 2.0002)\n",
      "Batch 490, Loss: 1.5020 (MSE: 1.0037, Aux: 2.0003)\n",
      "Batch 500, Loss: 1.5030 (MSE: 1.0056, Aux: 2.0003)\n",
      "Batch 510, Loss: 1.5080 (MSE: 1.0158, Aux: 2.0003)\n",
      "Batch 520, Loss: 1.5024 (MSE: 1.0041, Aux: 2.0007)\n",
      "Batch 530, Loss: 1.5098 (MSE: 1.0192, Aux: 2.0004)\n",
      "Batch 540, Loss: 1.5115 (MSE: 1.0225, Aux: 2.0005)\n",
      "Batch 550, Loss: 1.5054 (MSE: 1.0106, Aux: 2.0002)\n",
      "Batch 560, Loss: 1.4997 (MSE: 0.9990, Aux: 2.0004)\n",
      "Batch 570, Loss: 1.4980 (MSE: 0.9957, Aux: 2.0003)\n",
      "Batch 580, Loss: 1.5065 (MSE: 1.0128, Aux: 2.0002)\n",
      "Batch 590, Loss: 1.5063 (MSE: 1.0123, Aux: 2.0003)\n",
      "Batch 600, Loss: 1.5014 (MSE: 1.0021, Aux: 2.0006)\n",
      "Batch 610, Loss: 1.4947 (MSE: 0.9891, Aux: 2.0003)\n",
      "Batch 620, Loss: 1.5190 (MSE: 1.0376, Aux: 2.0005)\n",
      "Batch 630, Loss: 1.5092 (MSE: 1.0181, Aux: 2.0003)\n",
      "Batch 640, Loss: 1.4994 (MSE: 0.9985, Aux: 2.0003)\n",
      "Batch 650, Loss: 1.5000 (MSE: 0.9996, Aux: 2.0004)\n",
      "Batch 660, Loss: 1.5074 (MSE: 1.0146, Aux: 2.0003)\n",
      "Batch 670, Loss: 1.5034 (MSE: 1.0064, Aux: 2.0003)\n",
      "Batch 680, Loss: 1.5118 (MSE: 1.0233, Aux: 2.0003)\n",
      "Batch 690, Loss: 1.4976 (MSE: 0.9951, Aux: 2.0002)\n",
      "Batch 700, Loss: 1.4961 (MSE: 0.9921, Aux: 2.0002)\n",
      "Batch 710, Loss: 1.5025 (MSE: 1.0048, Aux: 2.0002)\n",
      "Batch 720, Loss: 1.5053 (MSE: 1.0104, Aux: 2.0002)\n",
      "Batch 730, Loss: 1.5113 (MSE: 1.0225, Aux: 2.0001)\n",
      "Batch 740, Loss: 1.5036 (MSE: 1.0070, Aux: 2.0003)\n",
      "Batch 750, Loss: 1.4928 (MSE: 0.9855, Aux: 2.0001)\n",
      "Batch 760, Loss: 1.5089 (MSE: 1.0177, Aux: 2.0001)\n",
      "Batch 770, Loss: 1.4981 (MSE: 0.9960, Aux: 2.0002)\n",
      "Batch 780, Loss: 1.5054 (MSE: 1.0106, Aux: 2.0002)\n",
      "Batch 790, Loss: 1.5031 (MSE: 1.0058, Aux: 2.0003)\n",
      "Batch 800, Loss: 1.5033 (MSE: 1.0063, Aux: 2.0003)\n",
      "Batch 810, Loss: 1.4961 (MSE: 0.9916, Aux: 2.0005)\n",
      "Batch 820, Loss: 1.4982 (MSE: 0.9963, Aux: 2.0001)\n",
      "Batch 830, Loss: 1.4969 (MSE: 0.9936, Aux: 2.0002)\n",
      "Batch 840, Loss: 1.5075 (MSE: 1.0146, Aux: 2.0003)\n",
      "Batch 850, Loss: 1.5056 (MSE: 1.0109, Aux: 2.0003)\n",
      "Batch 860, Loss: 1.5025 (MSE: 1.0049, Aux: 2.0002)\n",
      "Batch 870, Loss: 1.4991 (MSE: 0.9981, Aux: 2.0002)\n",
      "Batch 880, Loss: 1.5080 (MSE: 1.0158, Aux: 2.0002)\n",
      "Batch 890, Loss: 1.5109 (MSE: 1.0218, Aux: 2.0001)\n",
      "Batch 900, Loss: 1.5065 (MSE: 1.0128, Aux: 2.0002)\n",
      "Batch 910, Loss: 1.4957 (MSE: 0.9912, Aux: 2.0001)\n",
      "Batch 920, Loss: 1.5019 (MSE: 1.0038, Aux: 2.0001)\n",
      "Batch 930, Loss: 1.5071 (MSE: 1.0140, Aux: 2.0001)\n",
      "Batch 940, Loss: 1.5020 (MSE: 1.0039, Aux: 2.0002)\n",
      "Batch 950, Loss: 1.4954 (MSE: 0.9907, Aux: 2.0001)\n",
      "Batch 960, Loss: 1.5091 (MSE: 1.0180, Aux: 2.0001)\n",
      "Batch 970, Loss: 1.4907 (MSE: 0.9813, Aux: 2.0002)\n",
      "Batch 980, Loss: 1.5076 (MSE: 1.0151, Aux: 2.0001)\n",
      "Batch 990, Loss: 1.5069 (MSE: 1.0137, Aux: 2.0001)\n"
     ]
    }
   ],
   "source": [
    "def test_moe_training():\n",
    "    batch_size = 32\n",
    "    seq_len = 16\n",
    "    hidden_dim = 32\n",
    "    num_batches = 1000\n",
    "    \n",
    "    # Initialize model and optimizer\n",
    "    config = MOEConfig(hidden_dim=hidden_dim, \n",
    "                      expert_num=4,\n",
    "                      top_k=2,\n",
    "                      shared_expert_num=2)\n",
    "    model = SharedExpertMOE(config)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for batch in range(num_batches):\n",
    "        # Generate random input data\n",
    "        x = torch.randn(batch_size, seq_len, hidden_dim)\n",
    "        target = torch.randn(batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        # Forward pass\n",
    "        output, router_logits = model(x)\n",
    "\n",
    "        # Compute losses\n",
    "        # MSE loss for prediction\n",
    "        mse_loss = F.mse_loss(output, target)\n",
    "        \n",
    "        aux_loss = switch_load_balancing_loss(router_logits, config.expert_num, config.top_k)\n",
    "        # Combined loss\n",
    "        total_loss = 0.5 * mse_loss + 0.5 * aux_loss\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 10 == 0:\n",
    "            print(f\"Batch {batch}, Loss: {total_loss.item():.4f} \"\n",
    "                  f\"(MSE: {mse_loss.item():.4f}, Aux: {aux_loss.item():.4f})\")\n",
    "\n",
    "# Run the training test\n",
    "test_moe_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CogVideoX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
